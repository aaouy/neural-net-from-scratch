{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218380a6",
   "metadata": {},
   "source": [
    "# Neural Network from scratch to classify numbers\n",
    "\n",
    "The following jupyter notebook demonstrates a single hidden layer neural network built using only NumPy, and trained to classify numbers between 0 - 9. The MNIST dataset was used for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13160ec",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "53d4b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/alexouyang/.cache/kagglehub/datasets/hojjatk/mnist-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Download latest version\n",
    "dataset_path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "b10d0c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ['t10k-images-idx3-ubyte', 'train-labels.idx1-ubyte', 'train-images.idx3-ubyte', 't10k-labels-idx1-ubyte', 'train-images-idx3-ubyte', 't10k-labels.idx1-ubyte', 't10k-images.idx3-ubyte', 'train-labels-idx1-ubyte']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(dataset_path)\n",
    "print(\"Files:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "a97dd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_imgs = idx2numpy.convert_from_file(os.path.join(dataset_path, \"train-images.idx3-ubyte\"))\n",
    "raw_training_labels = idx2numpy.convert_from_file(os.path.join(dataset_path, \"train-labels.idx1-ubyte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "aef746de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "46a60858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(raw_training_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "cc529ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint8(5)"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "8d9e889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_labels(labels):\n",
    "    \"\"\"Converts an array of integer labels into a more suitable format for neural networks.\n",
    "\n",
    "    Args:\n",
    "        labels (np.ndarray): The labels of the training/testing set. Shape = (n_samples, ).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The labels of the training/testing set formatted as one-hot encoded vectors. Shape = (n_samples, 10).\n",
    "    \"\"\"\n",
    "    res = np.zeros((len(labels), 10), dtype=float)\n",
    "    res[np.arange(len(labels)), labels] = 1.0\n",
    "    return res \n",
    "\n",
    "def format_inputs(inputs):\n",
    "    \"\"\"Converts an array of inputs into a more suitable format for neural networks.\n",
    "\n",
    "    Args:\n",
    "        inputs (np.ndarray): The inputs of the training/testing set. Shape = (n_samples, n_rows, n_cols).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The inputs of the training/testing set formatted as 1D vectors. Shape = (n_samples, n_rows * n_cols).\n",
    "    \"\"\"\n",
    "    res = inputs.reshape((inputs.shape[0], inputs.shape[1] * inputs.shape[2])).copy()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "35bce103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], shape=(60000, 10))"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = format_labels(raw_training_labels)\n",
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "1827190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs = format_inputs(training_imgs)\n",
    "training_inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b35352",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "da4dff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The ReLU function.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The inputs to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The maximum of 0 and each input. Shape = (len(x), )\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The ReLU derivative.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): The input to the ReLU derivative.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: One if x > 0, otherwise zero. Shape = (len(x), )\n",
    "    \"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"The softmax function.\n",
    "\n",
    "    Args:\n",
    "        z (np.ndarray): The input to the softmax function.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalisation of inputs to a probability distribution. Shape = (len(z), )\n",
    "    \"\"\"\n",
    "    m = np.max(z)\n",
    "    return np.exp(z - m) / np.sum(np.exp(z - m))\n",
    "    \n",
    "def init_weights(n: int, m: int) -> np.ndarray:\n",
    "    \"\"\"Weight initialisation using He initialisation.\n",
    "\n",
    "    Args:\n",
    "        n (int): Size of origin layer.\n",
    "        m (int): Size of destination layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The weights from the origin to destination layer. Shape = (n, m)\n",
    "    \"\"\"\n",
    "    return np.random.normal(0, np.sqrt(2 / n), (n, m))\n",
    "\n",
    "def forward_pass(w1: np.ndarray, w2: np.ndarray, x: np.ndarray, b1: np.ndarray, b2: np.ndarray) -> tuple:\n",
    "    \"\"\"Forward pass of backpropagation.\n",
    "\n",
    "    Args:\n",
    "        w1 (np.ndarray): Weights of the input layer. Shape = (n_inputs, n_hidden).\n",
    "        w2 (np.ndarray): Weights of the hidden layer. Shape = (n_hidden, n_outputs).\n",
    "        x (np.ndarray): Inputs to the neural network. Shape = (n_inputs, )\n",
    "        b1 (np.ndarray): Bias of the input layer.\n",
    "        b2 (np.ndarray): Bias of the hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            z1 (np.ndarray): Weighted sum of the hidden layer.\n",
    "            z2 (np.ndarray): Weighted sum of the output layer.\n",
    "            a1 (np.ndarray): Inputs to the output layer.\n",
    "            a2 (np.ndarray): Outputs of the output layer.\n",
    "    \"\"\"\n",
    "    z1 = (x @ w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = (a1 @ w2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    return z1, z2, a1, a2\n",
    "    \n",
    "def backward_pass(x: np.ndarray, y: np.ndarray, a1: np.ndarray, a2: np.ndarray, z1: np.ndarray, w1: np.ndarray, w2: np.ndarray, b1: np.ndarray, b2: np.ndarray, lr: float) -> tuple:\n",
    "    \"\"\"Backward pass of backpropagation.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Inputs of the training set. Shape = (n_inputs, ).\n",
    "        y (np.ndarray): Labels of the training. Shape = (n_outputs, ).\n",
    "        a1 (np.ndarray): Inputs to the output layer. Shape = (n_hidden).\n",
    "        a2 (np.ndarray): Outputs of the output layer. Shape = (n_outputs, ).\n",
    "        z1 (np.ndarray): Weighted sum of the hidden layer. Shape = (n_hidden, ).\n",
    "        w1 (np.ndarray): Weights from the input to hidden layer. Shape = (n_inputs, n_hidden).\n",
    "        w2 (np.ndarray): Weights from the hidden to output layer. Shape = (n_hidden, n_output).\n",
    "        b1 (np.ndarray): Bias of the weighted sum at the hidden layer. Shape = (n_hidden, ).\n",
    "        b2 (np.ndarray): Bias of the weighted sum at the output layer. Shape = (n_output, ).\n",
    "        lr (float): The learning rate.\n",
    "\n",
    "    Returns:\n",
    "        float:\n",
    "            w1 (np.ndarray): New weights from input to hidden layer. Shape = (n_inputs, n_hidden).\n",
    "            w2 (np.ndarray): New weights from hidden to output layer. Shape = (n_hidden, n_output).\n",
    "            b1 (np.ndarray): Biases of weighted sum at hidden layer. Shape = (n_hidden, ).\n",
    "            b2 (np.ndarray): Biases of weighted sum at output layer. Shape = (n_output, ).\n",
    "    \"\"\"\n",
    "    delta2 = a2 - y \n",
    "    delta1 = relu_derivative(z1) * (w2 @ delta2)\n",
    "    \n",
    "    w2 -= (lr * np.outer(a1, delta2))\n",
    "    w1 -= (lr * np.outer(x, delta1))\n",
    "    \n",
    "    b2 -= lr * delta2\n",
    "    b1 -= lr * delta1\n",
    "    \n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "def backprop(x: np.ndarray, y: np.ndarray, w1: np.ndarray, w2: np.ndarray, b1: np.ndarray, b2: np.ndarray, lr: float) -> tuple:\n",
    "    \"\"\"Backpropagatio algorithm.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Inputs to the neural network. Shape = (n_inputs, )\n",
    "        y (np.ndarray): Labels of the neural network. Shape = (n_outpus, )\n",
    "        w1 (np.ndarray): Weights from the input to hidden layer. Shape = (n_inputs, n_hidden)\n",
    "        w2 (np.ndarray): Weights from the hidden to output layer. Shape = (n_hidden, n_output)\n",
    "        b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "        b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "        lr (float): The learning rate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            w1 (np.ndarray): Updated weights from the input to hidden layer. Shape = (n_inputs, n_hidden)\n",
    "            w2 (np.ndarray): Updated weights from the hidden to output layer. Shape = (n_hidden, n_output)\n",
    "            b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "            b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "    \"\"\"\n",
    "    z1, z2, a1, a2 = forward_pass(w1, w2, x, b1, b2)\n",
    "    w1, w2, b1, b2 = backward_pass(x, y, a1, a2, z1, w1, w2, b1, b2, lr)\n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "def train_nn(x: np.ndarray, y: np.ndarray, w1: np.ndarray, w2: np.ndarray, b1: np.ndarray, b2:np.ndarray, lr: float, epochs=1) -> tuple:\n",
    "    \"\"\"Train a neural network. \n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): Inputs to the neural network. Shape = (n_inputs, ).\n",
    "        y (np.ndarray): Labels of the neural network. Shape = (n_outpus, )\n",
    "        w1 (np.ndarray): Weights from the input to hidden layer. Shape = (n_inputs, n_hidden). \n",
    "        w2 (np.ndarray): Weights from the hidden to output layer. Shape = (n_hidden, n_output)\n",
    "        b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "        b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "        lr (float): The learning rate.\n",
    "        epochs (int, optional): Number of epochs to train for. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            w1 (np.ndarray): Updated weights from the input to hidden layer. Shape = (n_inputs, n_hidden)\n",
    "            w2 (np.ndarray): Updated weights from the hidden to output layer. Shape = (n_hidden, n_output)\n",
    "            b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "            b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    indices = np.arange(n)\n",
    "    for _ in range(epochs):\n",
    "        np.random.shuffle(indices)\n",
    "        x_train, y_train = x[indices], y[indices]\n",
    "        for i in range(n):\n",
    "            w1, w2, b1, b2 = backprop(x_train[i], y_train[i], w1, w2, b1, b2, lr)\n",
    "    return w1, w2, b1, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5ed07",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = training_inputs.shape[1]\n",
    "n2 = (n1 + 10) // 2\n",
    "\n",
    "w1 = init_weights(n1, n2)\n",
    "w2 = init_weights(n2, 10)\n",
    "\n",
    "b1 = np.zeros((n2, ), dtype=float)\n",
    "b2 = np.zeros((10, ), dtype=float)\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca4ce9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "3304244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_w1, trained_w2, trained_b1, trained_b2 = train_nn(training_inputs, training_labels, w1, w2, b1, b2, lr, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c684a24",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "19550af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_imgs = idx2numpy.convert_from_file(os.path.join(dataset_path, \"t10k-images.idx3-ubyte\"))\n",
    "raw_testing_labels = idx2numpy.convert_from_file(os.path.join(dataset_path, \"t10k-labels.idx1-ubyte\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "a83cc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels = format_labels(raw_testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "4ea4c0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "8e03951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_inputs = format_inputs(testing_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "0e6cda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, w1: np.ndarray, w2: np.ndarray, b1:np.ndarray, b2:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Predict a number.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): A single input to the neural network. Shape = (784, )\n",
    "        w1 (np.ndarray): Weights from the input to hidden layer. Shape = (n_inputs, n_hidden). \n",
    "        w2 (np.ndarray): Weights from the hidden to output layer. Shape = (n_hidden, n_output). \n",
    "        b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "        b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Probability distribution of the outputs.\n",
    "    \"\"\"\n",
    "    z1 = (x @ w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = (a1 @ w2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return a2\n",
    "    \n",
    "def validate_nn(x: np.ndarray, y: np.ndarray, w1: np.ndarray, w2: np.ndarray, b1: np.ndarray, b2: np.ndarray) -> float:\n",
    "    \"\"\" Testing o\n",
    "    Args:\n",
    "        x (np.ndarray): Inputs to the neural network. Shape = (n_inputs, ).\n",
    "        y (np.ndarray): Labels of the neural network. Shape = (n_outpus, ). \n",
    "        w1 (np.ndarray): Weights from the input to hidden layer. Shape = (n_inputs, n_hidden). \n",
    "        w2 (np.ndarray): Weights from the hidden to output layer. Shape = (n_hidden, n_output). \n",
    "        b1 (np.ndarray): Biases of the weighted sums in the hidden layer. Shape = (n_hidden, )\n",
    "        b2 (np.ndarray): Biases of the weighted sums in the output layer. Shape = (n_output, )\n",
    "    Returns:\n",
    "        float: Accuracy of the neural network on the test set.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    accuracy = 0\n",
    "    for i in range(n):\n",
    "        a2 = predict(x[i], w1, w2, b1, b2 )\n",
    "        if np.argmax(a2) == np.argmax(y[i]):\n",
    "            accuracy += 1\n",
    "    return accuracy / n\n",
    "\n",
    "def visualise_prediction(index: int, inputs: np.ndarray, labels: np.ndarray) -> None:\n",
    "    \"\"\"Visualise a prediction for a given input.\n",
    "\n",
    "    Args:\n",
    "        index (int): The index of the input and label pair to visualise.\n",
    "        inputs (np.ndarray): The input array (Can be training or testing set). Must be the same set as labels.\n",
    "        labels (np.ndarray): The label array (Can be training or testing set). Must be the same set as input.\n",
    "    \"\"\"\n",
    "    image = inputs[index]\n",
    "    label = labels[index]\n",
    "    \n",
    "    formatted_input = image.reshape(image.shape[0] * image.shape[1]).copy()\n",
    "    \n",
    "    prediction = predict(formatted_input, trained_w1, trained_w2, trained_b1, trained_b2)\n",
    "    print(\"Prediction: \", np.argmax(prediction))\n",
    "    print(\"Actual: \", label)\n",
    "    \n",
    "    plt.gray()\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "5b4e6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9234\n"
     ]
    }
   ],
   "source": [
    "accuracy = validate_nn(testing_inputs, testing_labels, trained_w1, trained_w2, trained_b1, trained_b2)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "8dc5ecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  5\n",
      "Actual:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_prediction(100, training_imgs, raw_training_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
